{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80efabf3",
   "metadata": {},
   "source": [
    "# How to use to generate LiveAvatar script\n",
    "\n",
    "1. First, you need a VLLM server running a compatible LLM. We recommend using gpt-oss 120b\n",
    "2. run `pip install transformers==4.41.1 torch torchaudio`\n",
    "3. run `bash demo/download_experimental_voices.sh`\n",
    "\n",
    "## Run cells in order to get .wav file script for LiveAvatar\n",
    "\n",
    "Output will be saved to this folder as `file_generated.wav`\n",
    "\n",
    "Once complete, navigate to run LiveAvatar using DigitalOcean Gradient 8 x NVIDIA H100 or 8 x NVIDIA H200 GPU Droplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de54412",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.41.1 torch torchaudio\n",
    "!bash demo/download_experimental_voices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df40d46-1717-4221-96af-0f529112c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[upbeat intro music]  \n",
      "Welcome to a quick dive into turning DigitalOcean’s 1‑Click Model GPU droplets into your very own voice‑enabled personal assistant.  \n",
      "\n",
      "First, what are 1‑Click Models? [curious tone] They’re a collaboration between DigitalOcean and Hugging Face that lets you spin up a cloud GPU droplet pre‑loaded with powerful open‑source LLMs—no coding required. Choose from models like Meta‑Llama‑3.1, Gemma‑2, Mixtral, or Hermes, all ready to run on the droplet’s GPU.  \n",
      "\n",
      "Once your droplet is up, you’ll see a Bearer Token in the SSH welcome message. [pause] Save that token—it’s the key to sending requests to the model’s inference API on port 8080, whether you’re on the same machine or a remote one.  \n",
      "\n",
      "You can query the model with a simple cURL command:  \n",
      "```bash\n",
      "curl http://localhost:8080/v1/chat/completions -X POST \\\n",
      "  -d '{\"messages\":[{\"role\":\"user\",\"content\":\"What is Deep Learning?\"}], \"temperature\":0.7,\"top_p\":0.95,\"max_tokens\":128}' \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
      "```  \n",
      "Or, use Python via the Hugging Face Hub client to get a ChatCompletion object.  \n",
      "\n",
      "Now, the fun part: building a voice‑first assistant. We combine three pieces:  \n",
      "1. **Whisper** for speech‑to‑text transcription,  \n",
      "2. The 1‑Click LLM for generating a response, and  \n",
      "3. **Coqui‑AI’s XTTS2** for text‑to‑speech, even cloning your own voice.  \n",
      "\n",
      "All of this is wrapped in a lightweight Gradio interface. The code launches a chatbot UI, lets you type or upload audio, and then streams the assistant’s spoken reply back to you.  \n",
      "\n",
      "To get it running, install the dependencies on your droplet:  \n",
      "```bash\n",
      "pip install gradio tts huggingface_hub transformers datasets scipy torch torchaudio\n",
      "```  \n",
      "Save the script as `app.py` and launch it with `python3 app.py`. Gradio will give you a shareable link, and you’ve got a full‑stack, cloud‑powered personal assistant—no proprietary API keys needed.  \n",
      "\n",
      "In our tests, this setup swaps out closed‑source tools like Gemini or ChatGPT, delivering comparable answers with the flexibility of open‑source models. It’s especially compelling for heavy‑weight LLMs like Mixtral‑8×22B or LLaMA‑405B, where renting a GPU droplet can be more cost‑effective than enterprise licenses.  \n",
      "\n",
      "So, whether you’re building a custom AI helper, a prototype, or just experimenting with the newest open‑source models, 1‑Click Model droplets make it surprisingly easy.  \n",
      "\n",
      "Thanks for watching! [cheerful outro music]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "title = 'Turning Your 1-Click Model GPU Droplets Into A Personal Assistant'\n",
    "article =\"\"\"\n",
    "1-Click Models are the new collaborative project from DigitalOcean and Hugging Face to bring you an easy method to interface with some of the best open-source Large Language Models (LLMs) on the most powerful GPUs available on the cloud. Together, users can optimize their usage of the best open-source models with no hassle or coding to setup.\n",
    "\n",
    "In this tutorial, we are going to show and walkthrough the development of a voice-enabled personal assistant tool designed to run on any 1-Click Model enabled GPU Droplet. This application uses Gradio, and is fully API enabled with FastAPI. Follow along to learn more about the advantages of using 1-Click Models, learn the basics of querying a deployed 1-Click Model GPU Droplet, and see how to use the personal assistant on your own machines!\n",
    "1-Click Hugging Face Models with DigitalOcean GPU Droplets\n",
    "\n",
    "The new 1-Click models come with a wide variety of LLM options, all with different use cases. These are namely:\n",
    "\n",
    "    meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "    meta-llama/Meta-Llama-3.1-70B-Instruct\n",
    "    meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\n",
    "    Qwen/Qwen2.5-7B-Instruct\n",
    "    google/gemma-2-9b-it\n",
    "    google/gemma-2-27b-it\n",
    "    mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "    mistralai/Mistral-7B-Instruct-v0.3\n",
    "    mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "    NousResearch/Hermes-3-Llama-3.1-8B\n",
    "    NousResearch/Hermes-3-Llama-3.1-70B\n",
    "    NousResearch/Hermes-3-Llama-3.1-405B\n",
    "    NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\n",
    "\n",
    "Creating a new GPU Droplet with any of these models only requires the process of setting up a GPU droplet, as shown here.\n",
    "\n",
    "Watch the following video for a full step-by-step guide to creating a 1-Click Model GPU Droplet, and check out this article for more details on launching a new instance.\n",
    "\n",
    "Once you have set up your new machine, navigate to the next section for more detail on interacting with your 1-Click Model.\n",
    "Interacting with the 1-Click Model Deployment\n",
    "\n",
    "Connecting to the 1-Click Model Deployment is simple if we want to interact with it on the same machine. “When connected to the HUGS Droplet, the initial SSH message will display a Bearer Token, which is required to send requests to the public IP of the deployed HUGS Droplet. Then you can send requests to the Messages API via either localhost if connected within the HUGS Droplet, or via its public IP.” (Source). To access the Droplet on other machines then, we will require getting the Bearer Token. Connect to your machine using SSH to get a copy of the token, and save it for later. If we are just wanting to interact with the inference endpoint from our GPU Droplet, things are pretty simple. The variable is already saved to the environment.\n",
    "\n",
    "Once the Bearer Token variable is set on the machine we are choosing to use, we can begin inferencing with the model. There are two routes to do this with at the moment: cURL and the Python. The endpoint will be automatically run from the port 8080, so we can default requests to our machine. If we are using a different machine, change the localhost value below to the IPv4 address.\n",
    "cURL\n",
    "\n",
    "curl http://localhost:8080/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\"messages\":[{\"role\":\"user\",\"content\":\"What is Deep Learning?\"}],\"temperature\":0.7,\"top_p\":0.95,\"max_tokens\":128}}' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
    "\n",
    "This code will ask the model “What is Deep Learning?” and issue a response in the following format:\n",
    "\n",
    "{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1731532721,\"model\":\"hfhugs/Meta-Llama-3.1-8B-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-169178b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"**Deep Learning: A Subfield of Machine Learning**\\n=====================================================\\n\\nDeep learning is a subfield of machine learning that focuses on the use of artificial neural networks to analyze and interpret data. It is inspired by the structure and function of the human brain and is particularly well-suited for tasks such as image and speech recognition, natural language processing, and data classification.\\n\\n**Key Characteristics of Deep Learning:**\\n\\n1. **Artificial Neural Networks**: Deep learning models are composed of multiple layers of interconnected nodes or \\\"neurons\\\" that process and transform inputs into outputs.\\n2. **Non-Linear Transformations**: Each layer applies a non-linear\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":40,\"completion_tokens\":128,\"total_tokens\":168}}\n",
    "\n",
    "This can then be plugged into a variety of web development applications as needed.\n",
    "Python\n",
    "\n",
    "The model can also Pythonically be accessed using either the Hugging Face Hub or OpenAI packages. We are going to refer to the Hugging Face Hub reference code for this demonstration.\n",
    "\n",
    "### Hugging Face Hub\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(base_url=\"http://localhost:8080\", api_key=os.getenv(\"BEARER_TOKEN\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":\"What is Deep Learning?\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,\n",
    ")\n",
    "\n",
    "This will return a formatted response as a ChatCompletionOutput object.\n",
    "\n",
    "## HuggingFace Hub\n",
    "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='**Deep Learning: An Overview**\\n\\nDeep Learning is a subset of Machine Learning that involves the use of Artificial Neural Networks (ANNs) with multiple layers to analyze and interpret data. These networks are inspired by the structure and function of the human brain, with each layer processing the input data in a hierarchical manner.\\n\\n**Key Characteristics:**\\n\\n1.  **Multiple Layers:** Deep Learning models typically have 2 or more hidden layers, allowing them to learn complex patterns and relationships in the data.\\n2.  **Neural Networks:** Deep Learning models are based on artificial neural networks, which are composed of interconnected nodes (neurons) that process', tool_calls=None), logprobs=None)], created=1731532948, id='', model='hfhugs/Meta-Llama-3.1-8B-Instruct', system_fingerprint='2.3.1-dev0-sha-169178b', usage=ChatCompletionOutputUsage(completion_tokens=128, prompt_tokens=40, total_tokens=168))\n",
    "\n",
    "We can print just the output with:\n",
    "\n",
    "chat_completion.choices[0]['message']['content']\n",
    "\n",
    "Interacting with the 1-Click Model Deployment\n",
    "\n",
    "Connecting to the 1-Click Model Deployment is simple if we want to interact with it on the same machine. “When connected to the HUGS Droplet, the initial SSH message will display a Bearer Token, which is required to send requests to the public IP of the deployed HUGS Droplet. Then you can send requests to the Messages API via either localhost if connected within the HUGS Droplet, or via its public IP.” (Source). To access the Droplet on other machines then, we will require getting the Bearer Token. Connect to your machine using SSH to get a copy of the token, and save it for later. If we are just wanting to interact with the inference endpoint from our GPU Droplet, things are pretty simple. The variable is already saved to the environment.\n",
    "\n",
    "Once the Bearer Token variable is set on the machine we are choosing to use, we can begin inferencing with the model. There are two routes to do this with at the moment: cURL and the Python. The endpoint will be automatically run from the port 8080, so we can default requests to our machine. If we are using a different machine, change the localhost value below to the IPv4 address.\n",
    "cURL\n",
    "\n",
    "curl http://localhost:8080/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\"messages\":[{\"role\":\"user\",\"content\":\"What is Deep Learning?\"}],\"temperature\":0.7,\"top_p\":0.95,\"max_tokens\":128}}' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
    "\n",
    "This code will ask the model “What is Deep Learning?” and issue a response in the following format:\n",
    "\n",
    "{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1731532721,\"model\":\"hfhugs/Meta-Llama-3.1-8B-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-169178b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"**Deep Learning: A Subfield of Machine Learning**\\n=====================================================\\n\\nDeep learning is a subfield of machine learning that focuses on the use of artificial neural networks to analyze and interpret data. It is inspired by the structure and function of the human brain and is particularly well-suited for tasks such as image and speech recognition, natural language processing, and data classification.\\n\\n**Key Characteristics of Deep Learning:**\\n\\n1. **Artificial Neural Networks**: Deep learning models are composed of multiple layers of interconnected nodes or \\\"neurons\\\" that process and transform inputs into outputs.\\n2. **Non-Linear Transformations**: Each layer applies a non-linear\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":40,\"completion_tokens\":128,\"total_tokens\":168}}\n",
    "\n",
    "This can then be plugged into a variety of web development applications as needed.\n",
    "Python\n",
    "\n",
    "The model can also Pythonically be accessed using either the Hugging Face Hub or OpenAI packages. We are going to refer to the Hugging Face Hub reference code for this demonstration.\n",
    "\n",
    "### Hugging Face Hub\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(base_url=\"http://localhost:8080\", api_key=os.getenv(\"BEARER_TOKEN\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":\"What is Deep Learning?\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,\n",
    ")\n",
    "\n",
    "This will return a formatted response as a ChatCompletionOutput object.\n",
    "\n",
    "## HuggingFace Hub\n",
    "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='**Deep Learning: An Overview**\\n\\nDeep Learning is a subset of Machine Learning that involves the use of Artificial Neural Networks (ANNs) with multiple layers to analyze and interpret data. These networks are inspired by the structure and function of the human brain, with each layer processing the input data in a hierarchical manner.\\n\\n**Key Characteristics:**\\n\\n1.  **Multiple Layers:** Deep Learning models typically have 2 or more hidden layers, allowing them to learn complex patterns and relationships in the data.\\n2.  **Neural Networks:** Deep Learning models are based on artificial neural networks, which are composed of interconnected nodes (neurons) that process', tool_calls=None), logprobs=None)], created=1731532948, id='', model='hfhugs/Meta-Llama-3.1-8B-Instruct', system_fingerprint='2.3.1-dev0-sha-169178b', usage=ChatCompletionOutputUsage(completion_tokens=128, prompt_tokens=40, total_tokens=168))\n",
    "\n",
    "We can print just the output with:\n",
    "\n",
    "chat_completion.choices[0]['message']['content']\n",
    "\n",
    "Interacting with the 1-Click Model Deployment\n",
    "\n",
    "Connecting to the 1-Click Model Deployment is simple if we want to interact with it on the same machine. “When connected to the HUGS Droplet, the initial SSH message will display a Bearer Token, which is required to send requests to the public IP of the deployed HUGS Droplet. Then you can send requests to the Messages API via either localhost if connected within the HUGS Droplet, or via its public IP.” (Source). To access the Droplet on other machines then, we will require getting the Bearer Token. Connect to your machine using SSH to get a copy of the token, and save it for later. If we are just wanting to interact with the inference endpoint from our GPU Droplet, things are pretty simple. The variable is already saved to the environment.\n",
    "\n",
    "Once the Bearer Token variable is set on the machine we are choosing to use, we can begin inferencing with the model. There are two routes to do this with at the moment: cURL and the Python. The endpoint will be automatically run from the port 8080, so we can default requests to our machine. If we are using a different machine, change the localhost value below to the IPv4 address.\n",
    "cURL\n",
    "\n",
    "curl http://localhost:8080/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\"messages\":[{\"role\":\"user\",\"content\":\"What is Deep Learning?\"}],\"temperature\":0.7,\"top_p\":0.95,\"max_tokens\":128}}' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
    "\n",
    "This code will ask the model “What is Deep Learning?” and issue a response in the following format:\n",
    "\n",
    "{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1731532721,\"model\":\"hfhugs/Meta-Llama-3.1-8B-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-169178b\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"**Deep Learning: A Subfield of Machine Learning**\\n=====================================================\\n\\nDeep learning is a subfield of machine learning that focuses on the use of artificial neural networks to analyze and interpret data. It is inspired by the structure and function of the human brain and is particularly well-suited for tasks such as image and speech recognition, natural language processing, and data classification.\\n\\n**Key Characteristics of Deep Learning:**\\n\\n1. **Artificial Neural Networks**: Deep learning models are composed of multiple layers of interconnected nodes or \\\"neurons\\\" that process and transform inputs into outputs.\\n2. **Non-Linear Transformations**: Each layer applies a non-linear\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":40,\"completion_tokens\":128,\"total_tokens\":168}}\n",
    "\n",
    "This can then be plugged into a variety of web development applications as needed.\n",
    "Python\n",
    "\n",
    "The model can also Pythonically be accessed using either the Hugging Face Hub or OpenAI packages. We are going to refer to the Hugging Face Hub reference code for this demonstration.\n",
    "\n",
    "### Hugging Face Hub\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(base_url=\"http://localhost:8080\", api_key=os.getenv(\"BEARER_TOKEN\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":\"What is Deep Learning?\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,\n",
    ")\n",
    "\n",
    "This will return a formatted response as a ChatCompletionOutput object.\n",
    "\n",
    "## HuggingFace Hub\n",
    "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='**Deep Learning: An Overview**\\n\\nDeep Learning is a subset of Machine Learning that involves the use of Artificial Neural Networks (ANNs) with multiple layers to analyze and interpret data. These networks are inspired by the structure and function of the human brain, with each layer processing the input data in a hierarchical manner.\\n\\n**Key Characteristics:**\\n\\n1.  **Multiple Layers:** Deep Learning models typically have 2 or more hidden layers, allowing them to learn complex patterns and relationships in the data.\\n2.  **Neural Networks:** Deep Learning models are based on artificial neural networks, which are composed of interconnected nodes (neurons) that process', tool_calls=None), logprobs=None)], created=1731532948, id='', model='hfhugs/Meta-Llama-3.1-8B-Instruct', system_fingerprint='2.3.1-dev0-sha-169178b', usage=ChatCompletionOutputUsage(completion_tokens=128, prompt_tokens=40, total_tokens=168))\n",
    "\n",
    "We can print just the output with:\n",
    "\n",
    "chat_completion.choices[0]['message']['content']\n",
    "\n",
    "Creating a Voice Enabled Personal Assistant\n",
    "\n",
    "To make the best use of this powerful new tool, we have developed a new personal assistant application to run with the models. The application is fully voice enabled, capable of listening to and reading back out loud inputs and outputs. To make this possible, the demo uses Whisper to transcribe an audio input, or takes plain text, and inputs that to an the LLM powered by 1-Click GPU Droplets to generate a text response. We then use Coqui-AI’s XTTS2 model to convert the text input into a understandable audio output. It’s worth noting that the software uses voice cloning to generate the output audio, so users will receive a voice output close to their own speaking voice.\n",
    "\n",
    "Take a look at the code below:\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id_w = \"openai/whisper-large-v3\"\n",
    "\n",
    "model_w = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id_w, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model_w.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id_w)\n",
    "\n",
    "pipe_w = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_w,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "client = InferenceClient(base_url=\"http://localhost:8080\", api_key=os.getenv(\"BEARER_TOKEN\"))\n",
    "\n",
    "# Example voice cloning with YourTTS in English, French and Portuguese\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/bark\", gpu=True)\n",
    "\n",
    "# get v2.0.2\n",
    "tts = TTS(model_name=\"xtts_v2.0.2\", gpu=True)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label = 'Prompt')\n",
    "        audi = gr.Audio(label = 'Transcribe audio')\n",
    "    with gr.Row():\n",
    "        submit = gr.Button('Submit')\n",
    "        submit_audio = gr.Button('Submit Audio')\n",
    "        read_audio = gr.Button('Transcribe Text to Audio')\n",
    "        clear = gr.ClearButton([msg, chatbot])\n",
    "    with gr.Row():\n",
    "        token_val = gr.Slider(label = 'Max new tokens', value = 512, minimum = 128, maximum = 1024, step = 8, interactive=True)\n",
    "        temperature_ = gr.Slider(label = 'Temperature', value = .7, minimum = 0, maximum =1, step = .1, interactive=True)\n",
    "        top_p_ = gr.Slider(label = 'Top P', value = .95, minimum = 0, maximum =1, step = .05, interactive=True)\n",
    "\n",
    "    def respond(message, chat_history, token_val, temperature_, top_p_):\n",
    "        bot_message = client.chat.completions.create(messages=[{\"role\":\"user\",\"content\":f\"{message}\"},],temperature=temperature_,top_p=top_p_,max_tokens=token_val,).choices[0]['message']['content']\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        # tts.tts_to_file(bot_message, speaker_wav=\"output.wav\", language=\"en\", file_path=\"output.wav\")\n",
    "\n",
    "        return \"\", chat_history, #\"output.wav\"\n",
    "    \n",
    "    def respond_audio(audi, chat_history, token_val, temperature_, top_p_):  \n",
    "        wavfile.write(\"output.wav\", 44100, audi[1]) \n",
    "        result = pipe_w('output.wav')\n",
    "        message = result[\"text\"]\n",
    "        print(message)\n",
    "        bot_message = client.chat.completions.create(messages=[{\"role\":\"user\",\"content\":f\"{message}\"},],temperature=temperature_,top_p=top_p_,max_tokens=token_val,).choices[0]['message']['content']\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        # tts.tts_to_file(bot_message, speaker_wav=\"output.wav\", language=\"en\", file_path=\"output2.wav\")\n",
    "        # tts.tts_to_file(bot_message,\n",
    "                # file_path=\"output.wav\",\n",
    "                # speaker_wav=\"output.wav\",\n",
    "                # language=\"en\")\n",
    "        return \"\", chat_history, #\"output.wav\"\n",
    "    def read_text(chat_history):\n",
    "        print(chat_history)\n",
    "        print(type(chat_history))\n",
    "        tts.tts_to_file(chat_history[-1]['content'],\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"output.wav\",\n",
    "                language=\"en\")\n",
    "        return 'output.wav'\n",
    "\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    submit.click(respond, [msg, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    submit_audio.click(respond_audio, [audi, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    read_audio.click(read_text, [chatbot], [audi])\n",
    "demo.launch(share = True)\n",
    "\n",
    "Put together, this integrated system makes it possible to take full advantage of the speed and availability of a cloud GPU to act as a personal assistant for all kinds of tasks. We have been using it in place of popular closed source tools like Gemini and ChatGPT, and have really been impressed with the results.\n",
    "Setting up & Running the Demo\n",
    "\n",
    "To install the required packages onto your GPU Droplet, paste the following into the terminal:\n",
    "\n",
    "pip install gradio tts huggingface_hub transformers datasets scipy torch torchaudio\n",
    "\n",
    "To run this demo, simply paste the code above into a blank python file (let’s arbitrarily call it app.py) on your 1-Click Model enabled Cloud GPU, and run it with python3 app.py.\n",
    "Creating a Voice Enabled Personal Assistant\n",
    "\n",
    "To make the best use of this powerful new tool, we have developed a new personal assistant application to run with the models. The application is fully voice enabled, capable of listening to and reading back out loud inputs and outputs. To make this possible, the demo uses Whisper to transcribe an audio input, or takes plain text, and inputs that to an the LLM powered by 1-Click GPU Droplets to generate a text response. We then use Coqui-AI’s XTTS2 model to convert the text input into a understandable audio output. It’s worth noting that the software uses voice cloning to generate the output audio, so users will receive a voice output close to their own speaking voice.\n",
    "\n",
    "Take a look at the code below:\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id_w = \"openai/whisper-large-v3\"\n",
    "\n",
    "model_w = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id_w, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model_w.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id_w)\n",
    "\n",
    "pipe_w = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_w,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "client = InferenceClient(base_url=\"http://localhost:8080\", api_key=os.getenv(\"BEARER_TOKEN\"))\n",
    "\n",
    "# Example voice cloning with YourTTS in English, French and Portuguese\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/bark\", gpu=True)\n",
    "\n",
    "# get v2.0.2\n",
    "tts = TTS(model_name=\"xtts_v2.0.2\", gpu=True)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(label = 'Prompt')\n",
    "        audi = gr.Audio(label = 'Transcribe audio')\n",
    "    with gr.Row():\n",
    "        submit = gr.Button('Submit')\n",
    "        submit_audio = gr.Button('Submit Audio')\n",
    "        read_audio = gr.Button('Transcribe Text to Audio')\n",
    "        clear = gr.ClearButton([msg, chatbot])\n",
    "    with gr.Row():\n",
    "        token_val = gr.Slider(label = 'Max new tokens', value = 512, minimum = 128, maximum = 1024, step = 8, interactive=True)\n",
    "        temperature_ = gr.Slider(label = 'Temperature', value = .7, minimum = 0, maximum =1, step = .1, interactive=True)\n",
    "        top_p_ = gr.Slider(label = 'Top P', value = .95, minimum = 0, maximum =1, step = .05, interactive=True)\n",
    "\n",
    "    def respond(message, chat_history, token_val, temperature_, top_p_):\n",
    "        bot_message = client.chat.completions.create(messages=[{\"role\":\"user\",\"content\":f\"{message}\"},],temperature=temperature_,top_p=top_p_,max_tokens=token_val,).choices[0]['message']['content']\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        # tts.tts_to_file(bot_message, speaker_wav=\"output.wav\", language=\"en\", file_path=\"output.wav\")\n",
    "\n",
    "        return \"\", chat_history, #\"output.wav\"\n",
    "    \n",
    "    def respond_audio(audi, chat_history, token_val, temperature_, top_p_):  \n",
    "        wavfile.write(\"output.wav\", 44100, audi[1]) \n",
    "        result = pipe_w('output.wav')\n",
    "        message = result[\"text\"]\n",
    "        print(message)\n",
    "        bot_message = client.chat.completions.create(messages=[{\"role\":\"user\",\"content\":f\"{message}\"},],temperature=temperature_,top_p=top_p_,max_tokens=token_val,).choices[0]['message']['content']\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        # tts.tts_to_file(bot_message, speaker_wav=\"output.wav\", language=\"en\", file_path=\"output2.wav\")\n",
    "        # tts.tts_to_file(bot_message,\n",
    "                # file_path=\"output.wav\",\n",
    "                # speaker_wav=\"output.wav\",\n",
    "                # language=\"en\")\n",
    "        return \"\", chat_history, #\"output.wav\"\n",
    "    def read_text(chat_history):\n",
    "        print(chat_history)\n",
    "        print(type(chat_history))\n",
    "        tts.tts_to_file(chat_history[-1]['content'],\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"output.wav\",\n",
    "                language=\"en\")\n",
    "        return 'output.wav'\n",
    "\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    submit.click(respond, [msg, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    submit_audio.click(respond_audio, [audi, chatbot, token_val, temperature_, top_p_], [msg, chatbot])\n",
    "    read_audio.click(read_text, [chatbot], [audi])\n",
    "demo.launch(share = True)\n",
    "\n",
    "Put together, this integrated system makes it possible to take full advantage of the speed and availability of a cloud GPU to act as a personal assistant for all kinds of tasks. We have been using it in place of popular closed source tools like Gemini and ChatGPT, and have really been impressed with the results.\n",
    "Setting up & Running the Demo\n",
    "\n",
    "To install the required packages onto your GPU Droplet, paste the following into the terminal:\n",
    "\n",
    "pip install gradio tts huggingface_hub transformers datasets scipy torch torchaudio\n",
    "\n",
    "To run this demo, simply paste the code above into a blank python file (let’s arbitrarily call it app.py) on your 1-Click Model enabled Cloud GPU, and run it with python3 app.py.\n",
    "Closing Thoughts\n",
    "\n",
    "The personal assistant application developed for this tutorial has already proven useful for us in our daily lives, and we hope others can find some utility using them. Furthermore, the new 1-Click Model GPU Droplets offer a really interesting alternative to enterprise LLM software. While costly for single users, there are a number of use cases we can think of (namely running the largest open-source LLMs) that can justify the expenditure. Our new offerings have the largest Mixtral and LLaMA models available, so it is an interesting opportunity to test the power of these models against the best competition.\n",
    "\n",
    "Thank you for reading!\n",
    "\n",
    "\"\"\"\n",
    " \n",
    "result = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for creating short video essay scripts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Extract the content from the following article (which includes all text following the colon), and create a short summary script for a video explaining the article. Do not include any additional text for the script, only the words that are to be spoken. The article is titled {title}, and its content is: {article}\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(result.choices[0].message.content)\n",
    "text = str(result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab288e46-830b-4b29-bd89-834c6e0bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.txt', 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9b5c2a-499e-439d-98cd-877956ec5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APEX FusedRMSNorm not available, using native implementation\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from typing import List, Tuple, Union, Dict, Any\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference\n",
    "from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor\n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class VoiceMapper:\n",
    "    \"\"\"Maps speaker names to voice file paths\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_voice_presets()\n",
    "        for k, v in self.voice_presets.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    def setup_voice_presets(self):\n",
    "        \"\"\"Setup voice presets by scanning the voices directory.\"\"\"\n",
    "        voices_dir = os.path.join(os.path.dirname('./demo/'), \"voices/streaming_model\")\n",
    "        \n",
    "        # Check if voices directory exists\n",
    "        if not os.path.exists(voices_dir):\n",
    "            print(f\"Warning: Voices directory not found at {voices_dir}\")\n",
    "            self.voice_presets = {}\n",
    "            self.available_voices = {}\n",
    "            return\n",
    "        \n",
    "        # Scan for all VOICE files in the voices directory\n",
    "        self.voice_presets = {}\n",
    "        \n",
    "        # Get all .pt files in the voices directory\n",
    "        pt_files = glob.glob(os.path.join(voices_dir, \"**\", \"*.pt\"), recursive=True)\n",
    "        \n",
    "        # Create dictionary with filename (without extension) as key\n",
    "        for pt_file in pt_files:\n",
    "            # key: filename without extension\n",
    "            name = os.path.splitext(os.path.basename(pt_file))[0].lower()\n",
    "            full_path = os.path.abspath(pt_file)\n",
    "            self.voice_presets[name] = full_path\n",
    "        \n",
    "        # Sort the voice presets alphabetically by name for better UI\n",
    "        self.voice_presets = dict(sorted(self.voice_presets.items()))\n",
    "        \n",
    "        # Filter out voices that don't exist (this is now redundant but kept for safety)\n",
    "        self.available_voices = {\n",
    "            name: path for name, path in self.voice_presets.items()\n",
    "            if os.path.exists(path)\n",
    "        }\n",
    "        \n",
    "        print(f\"Found {len(self.available_voices)} voice files in {voices_dir}\")\n",
    "        print(f\"Available voices: {', '.join(self.available_voices.keys())}\")\n",
    "\n",
    "    def get_voice_path(self, speaker_name: str) -> str:\n",
    "        \"\"\"Get voice file path for a given speaker name\"\"\"\n",
    "        # First try exact match\n",
    "        speaker_name = speaker_name.lower()\n",
    "        if speaker_name in self.voice_presets:\n",
    "            return self.voice_presets[speaker_name]\n",
    "        \n",
    "        # Try partial matching (case insensitive)\n",
    "        matched_path = None\n",
    "        for preset_name, path in self.voice_presets.items():\n",
    "            if preset_name.lower() in speaker_name or speaker_name in preset_name.lower():\n",
    "                if matched_path is not None:\n",
    "                    raise ValueError(f\"Multiple voice presets match the speaker name '{speaker_name}', please make the speaker_name more specific.\")\n",
    "                matched_path = path\n",
    "        if matched_path is not None:\n",
    "            return matched_path\n",
    "        \n",
    "        # Default to first voice if no match found\n",
    "        default_voice = list(self.voice_presets.values())[0]\n",
    "        print(f\"Warning: No voice preset found for '{speaker_name}', using default voice: {default_voice}\")\n",
    "        return default_voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183f2324-68ff-4c4a-8ac1-9a9438140941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
      "The class this function is called from is 'VibeVoiceTextTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 voice files in ./demo/voices/streaming_model\n",
      "Available voices: de-spk0_man, de-spk1_woman, de-spk2_woman, de-spk3_man, de-spk4_woman, de-spk5_man, de-spk6_man, en-breeze_woman, en-brutalon_man, en-carter_man, en-clarion_man, en-clarissa_woman, en-davis_man, en-emma_woman, en-frank_man, en-grace_woman, en-gravitar_man, en-gravus_man, en-mechcorsair_man, en-mike_man, en-oldenheart_man, en-silkvox_man, en-snarkling_woman, en-soother_woman, fr-spk0_man, fr-spk1_woman, fr-spk2_man, fr-spk3_woman, fr-spk4_woman, fr-spk5_man, in-samuel_man, it-spk0_woman, it-spk1_man, jp-spk0_man, jp-spk1_woman, jp-spk2_woman, jp-spk3_woman, jp-spk4_woman, jp-spk5_man, kr-spk0_woman, kr-spk1_man, kr-spk2_woman, kr-spk3_man, nl-spk0_man, nl-spk1_woman, pl-spk0_man, pl-spk1_woman, pl-spk2_man, pl-spk3_woman, pt-spk0_woman, pt-spk1_man, pt-spk2_woman, pt-spk3_man, pt-spk4_man, pt-spk5_woman, sp-spk0_woman, sp-spk1_man, sp-spk2_woman, sp-spk3_man, sp-spk4_woman, sp-spk5_man\n",
      "de-spk0_man: /home/VibeVoice/demo/voices/streaming_model/de-Spk0_man.pt\n",
      "de-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/de-Spk1_woman.pt\n",
      "de-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk2_woman.pt\n",
      "de-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk3_man.pt\n",
      "de-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk4_woman.pt\n",
      "de-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk5_man.pt\n",
      "de-spk6_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk6_man.pt\n",
      "en-breeze_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Breeze_woman.pt\n",
      "en-brutalon_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Brutalon_man.pt\n",
      "en-carter_man: /home/VibeVoice/demo/voices/streaming_model/en-Carter_man.pt\n",
      "en-clarion_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Clarion_man.pt\n",
      "en-clarissa_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Clarissa_woman.pt\n",
      "en-davis_man: /home/VibeVoice/demo/voices/streaming_model/en-Davis_man.pt\n",
      "en-emma_woman: /home/VibeVoice/demo/voices/streaming_model/en-Emma_woman.pt\n",
      "en-frank_man: /home/VibeVoice/demo/voices/streaming_model/en-Frank_man.pt\n",
      "en-grace_woman: /home/VibeVoice/demo/voices/streaming_model/en-Grace_woman.pt\n",
      "en-gravitar_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Gravitar_man.pt\n",
      "en-gravus_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Gravus_man.pt\n",
      "en-mechcorsair_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-MechCorsair_man.pt\n",
      "en-mike_man: /home/VibeVoice/demo/voices/streaming_model/en-Mike_man.pt\n",
      "en-oldenheart_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Oldenheart_man.pt\n",
      "en-silkvox_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Silkvox_man.pt\n",
      "en-snarkling_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Snarkling_woman.pt\n",
      "en-soother_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Soother_woman.pt\n",
      "fr-spk0_man: /home/VibeVoice/demo/voices/streaming_model/fr-Spk0_man.pt\n",
      "fr-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/fr-Spk1_woman.pt\n",
      "fr-spk2_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk2_man.pt\n",
      "fr-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk3_woman.pt\n",
      "fr-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk4_woman.pt\n",
      "fr-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk5_man.pt\n",
      "in-samuel_man: /home/VibeVoice/demo/voices/streaming_model/in-Samuel_man.pt\n",
      "it-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/it-Spk0_woman.pt\n",
      "it-spk1_man: /home/VibeVoice/demo/voices/streaming_model/it-Spk1_man.pt\n",
      "jp-spk0_man: /home/VibeVoice/demo/voices/streaming_model/jp-Spk0_man.pt\n",
      "jp-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/jp-Spk1_woman.pt\n",
      "jp-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk2_woman.pt\n",
      "jp-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk3_woman.pt\n",
      "jp-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk4_woman.pt\n",
      "jp-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk5_man.pt\n",
      "kr-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/kr-Spk0_woman.pt\n",
      "kr-spk1_man: /home/VibeVoice/demo/voices/streaming_model/kr-Spk1_man.pt\n",
      "kr-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/kr/kr-Spk2_woman.pt\n",
      "kr-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/kr/kr-Spk3_man.pt\n",
      "nl-spk0_man: /home/VibeVoice/demo/voices/streaming_model/nl-Spk0_man.pt\n",
      "nl-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/nl-Spk1_woman.pt\n",
      "pl-spk0_man: /home/VibeVoice/demo/voices/streaming_model/pl-Spk0_man.pt\n",
      "pl-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/pl-Spk1_woman.pt\n",
      "pl-spk2_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pl/pl-Spk2_man.pt\n",
      "pl-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pl/pl-Spk3_woman.pt\n",
      "pt-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/pt-Spk0_woman.pt\n",
      "pt-spk1_man: /home/VibeVoice/demo/voices/streaming_model/pt-Spk1_man.pt\n",
      "pt-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk2_woman.pt\n",
      "pt-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk3_man.pt\n",
      "pt-spk4_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk4_man.pt\n",
      "pt-spk5_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk5_woman.pt\n",
      "sp-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/sp-Spk0_woman.pt\n",
      "sp-spk1_man: /home/VibeVoice/demo/voices/streaming_model/sp-Spk1_man.pt\n",
      "sp-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk2_woman.pt\n",
      "sp-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk3_man.pt\n",
      "sp-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk4_woman.pt\n",
      "sp-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk5_man.pt\n",
      "Reading script from: file.txt\n",
      "Loading processor & model from microsoft/VibeVoice-Realtime-0.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/config.json\n",
      "Model config VibeVoiceStreamingConfig {\n",
      "  \"acoustic_tokenizer_config\": {\n",
      "    \"causal\": true,\n",
      "    \"channels\": 1,\n",
      "    \"conv_bias\": true,\n",
      "    \"conv_norm\": \"none\",\n",
      "    \"corpus_normalize\": 0.0,\n",
      "    \"decoder_depths\": null,\n",
      "    \"decoder_n_filters\": 32,\n",
      "    \"decoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"disable_last_norm\": true,\n",
      "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
      "    \"encoder_n_filters\": 32,\n",
      "    \"encoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"fix_std\": 0.5,\n",
      "    \"layer_scale_init_value\": 1e-06,\n",
      "    \"layernorm\": \"RMSNorm\",\n",
      "    \"layernorm_elementwise_affine\": true,\n",
      "    \"layernorm_eps\": 1e-05,\n",
      "    \"mixer_layer\": \"depthwise_conv\",\n",
      "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
      "    \"pad_mode\": \"constant\",\n",
      "    \"std_dist_type\": \"gaussian\",\n",
      "    \"vae_dim\": 64,\n",
      "    \"weight_init_value\": 0.01\n",
      "  },\n",
      "  \"acoustic_vae_dim\": 64,\n",
      "  \"architectures\": [\n",
      "    \"VibeVoiceStreamingForConditionalGenerationInference\"\n",
      "  ],\n",
      "  \"decoder_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 896,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4864,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"max_window_layers\": 24,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"num_attention_heads\": 14,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"diffusion_head_config\": {\n",
      "    \"ddpm_batch_mul\": 4,\n",
      "    \"ddpm_beta_schedule\": \"cosine\",\n",
      "    \"ddpm_num_inference_steps\": 20,\n",
      "    \"ddpm_num_steps\": 1000,\n",
      "    \"diffusion_type\": \"ddpm\",\n",
      "    \"head_ffn_ratio\": 3.0,\n",
      "    \"head_layers\": 4,\n",
      "    \"hidden_size\": 896,\n",
      "    \"latent_size\": 64,\n",
      "    \"model_type\": \"vibevoice_diffusion_head\",\n",
      "    \"prediction_type\": \"v_prediction\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"speech_vae_dim\": 64\n",
      "  },\n",
      "  \"model_type\": \"vibevoice_streaming\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tts_backbone_num_hidden_layers\": 20\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/model.safetensors\n",
      "Instantiating VibeVoiceStreamingForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/config.json\n",
      "Model config VibeVoiceStreamingConfig {\n",
      "  \"acoustic_tokenizer_config\": {\n",
      "    \"causal\": true,\n",
      "    \"channels\": 1,\n",
      "    \"conv_bias\": true,\n",
      "    \"conv_norm\": \"none\",\n",
      "    \"corpus_normalize\": 0.0,\n",
      "    \"decoder_depths\": null,\n",
      "    \"decoder_n_filters\": 32,\n",
      "    \"decoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"disable_last_norm\": true,\n",
      "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
      "    \"encoder_n_filters\": 32,\n",
      "    \"encoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"fix_std\": 0.5,\n",
      "    \"layer_scale_init_value\": 1e-06,\n",
      "    \"layernorm\": \"RMSNorm\",\n",
      "    \"layernorm_elementwise_affine\": true,\n",
      "    \"layernorm_eps\": 1e-05,\n",
      "    \"mixer_layer\": \"depthwise_conv\",\n",
      "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
      "    \"pad_mode\": \"constant\",\n",
      "    \"std_dist_type\": \"gaussian\",\n",
      "    \"vae_dim\": 64,\n",
      "    \"weight_init_value\": 0.01\n",
      "  },\n",
      "  \"acoustic_vae_dim\": 64,\n",
      "  \"architectures\": [\n",
      "    \"VibeVoiceStreamingForConditionalGenerationInference\"\n",
      "  ],\n",
      "  \"decoder_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 896,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4864,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"max_window_layers\": 24,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"num_attention_heads\": 14,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"diffusion_head_config\": {\n",
      "    \"ddpm_batch_mul\": 4,\n",
      "    \"ddpm_beta_schedule\": \"cosine\",\n",
      "    \"ddpm_num_inference_steps\": 20,\n",
      "    \"ddpm_num_steps\": 1000,\n",
      "    \"diffusion_type\": \"ddpm\",\n",
      "    \"head_ffn_ratio\": 3.0,\n",
      "    \"head_layers\": 4,\n",
      "    \"hidden_size\": 896,\n",
      "    \"latent_size\": 64,\n",
      "    \"model_type\": \"vibevoice_diffusion_head\",\n",
      "    \"prediction_type\": \"v_prediction\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"speech_vae_dim\": 64\n",
      "  },\n",
      "  \"model_type\": \"vibevoice_streaming\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tts_backbone_num_hidden_layers\": 20\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/model.safetensors\n",
      "Instantiating VibeVoiceStreamingForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {}\n",
      "\n",
      "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
      "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
      "Instantiating VibeVoiceAcousticTokenizerModel model under default dtype torch.bfloat16.\n",
      "Instantiating VibeVoiceDiffusionHead model under default dtype torch.bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, torch_dtype: torch.bfloat16, attn_implementation: flash_attention_2\n",
      "[ERROR] : ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_437806/4057761752.py\", line 50, in <module>\n",
      "    model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4336, in from_pretrained\n",
      "    config = cls._autoset_attn_implementation(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
      "    cls._check_and_enable_flash_attn_2(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
      "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
      "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "\n",
      "Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing VibeVoiceStreamingForConditionalGenerationInference.\n",
      "\n",
      "All the weights of VibeVoiceStreamingForConditionalGenerationInference were initialized from the model checkpoint at microsoft/VibeVoice-Realtime-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use VibeVoiceStreamingForConditionalGenerationInference for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model attention: sdpa\n",
      "Using voice preset for en-carter_man: /home/VibeVoice/demo/voices/streaming_model/en-Carter_man.pt\n",
      "Starting generation with cfg_scale: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 83.14 seconds\n",
      "Generated audio duration: 266.40 seconds\n",
      "RTF (Real Time Factor): 0.31x\n",
      "Prefilling text tokens: 646\n",
      "Generated speech tokens: 1998\n",
      "Total tokens: 2960\n",
      "Saved output to ./file_generated.wav\n",
      "\n",
      "==================================================\n",
      "GENERATION SUMMARY\n",
      "==================================================\n",
      "Input file: file.txt\n",
      "Output file: ./file_generated.wav\n",
      "Speaker names: en-carter_man\n",
      "Prefilling text tokens: 646\n",
      "Generated speech tokens: 1998\n",
      "Total tokens: 2960\n",
      "Generation time: 83.14 seconds\n",
      "Audio duration: 266.40 seconds\n",
      "RTF (Real Time Factor): 0.31x\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = \"microsoft/VibeVoice-Realtime-0.5B\"\n",
    "txt_path = \"file.txt\"\n",
    "speaker_name = \"en-carter_man\"\n",
    "ouput_dir = \"./outputs\"\n",
    "device = 'cuda'\n",
    "cfg_scale = 1.5\n",
    "output_dir = './'\n",
    "# Initialize voice mapper\n",
    "voice_mapper = VoiceMapper()\n",
    "\n",
    "# Check if txt file exists\n",
    "if not os.path.exists(txt_path):\n",
    "    print(f\"Error: txt file not found: {txt_path}\")\n",
    "\n",
    "# Read and parse txt file\n",
    "print(f\"Reading script from: {txt_path}\")\n",
    "with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "    scripts = f.read().strip()\n",
    "\n",
    "if not scripts:\n",
    "    print(\"Error: No valid scripts found in the txt file\")\n",
    "\n",
    "full_script = scripts.replace(\"’\", \"'\").replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "print(f\"Loading processor & model from {model_path}\")\n",
    "processor = VibeVoiceStreamingProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Decide dtype & attention implementation\n",
    "if device == \"mps\":\n",
    "    load_dtype = torch.float32  # MPS requires float32\n",
    "    attn_impl_primary = \"sdpa\"  # flash_attention_2 not supported on MPS\n",
    "elif device == \"cuda\":\n",
    "    load_dtype = torch.bfloat16\n",
    "    attn_impl_primary = \"flash_attention_2\"\n",
    "else:  # cpu\n",
    "    load_dtype = torch.float32\n",
    "    attn_impl_primary = \"sdpa\"\n",
    "print(f\"Using device: {device}, torch_dtype: {load_dtype}, attn_implementation: {attn_impl_primary}\")\n",
    "# Load model with device-specific logic\n",
    "try:\n",
    "    if device == \"mps\":\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            attn_implementation=attn_impl_primary,\n",
    "            device_map=None,  # load then move\n",
    "        )\n",
    "        model.to(\"mps\")\n",
    "    elif device == \"cuda\":\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=\"cuda\",\n",
    "            attn_implementation=attn_impl_primary,\n",
    "        )\n",
    "    else:  # cpu\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=\"cpu\",\n",
    "            attn_implementation=attn_impl_primary,\n",
    "        )\n",
    "except Exception as e:\n",
    "    if attn_impl_primary == 'flash_attention_2':\n",
    "        print(f\"[ERROR] : {type(e).__name__}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.\")\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=(device if device in (\"cuda\", \"cpu\") else None),\n",
    "            attn_implementation='sdpa'\n",
    "        )\n",
    "        if device == \"mps\":\n",
    "            model.to(\"mps\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.set_ddpm_inference_steps(num_steps=5)\n",
    "\n",
    "if hasattr(model.model, 'language_model'):\n",
    "   print(f\"Language model attention: {model.model.language_model.config._attn_implementation}\")\n",
    "\n",
    "target_device = device if device != \"cpu\" else \"cpu\"\n",
    "voice_sample = voice_mapper.get_voice_path(speaker_name)\n",
    "print(f\"Using voice preset for {speaker_name}: {voice_sample}\")\n",
    "all_prefilled_outputs = torch.load(voice_sample, map_location=target_device, weights_only=False)\n",
    "\n",
    "# Prepare inputs for the model\n",
    "inputs = processor.process_input_with_cached_prompt(\n",
    "    text=full_script,\n",
    "    cached_prompt=all_prefilled_outputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "# Move tensors to target device\n",
    "for k, v in inputs.items():\n",
    "    if torch.is_tensor(v):\n",
    "        inputs[k] = v.to(target_device)\n",
    "\n",
    "print(f\"Starting generation with cfg_scale: {cfg_scale}\")\n",
    "\n",
    "# Generate audio\n",
    "start_time = time.time()\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=None,\n",
    "    cfg_scale=cfg_scale,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    generation_config={'do_sample': False},\n",
    "    verbose=True,\n",
    "    all_prefilled_outputs=copy.deepcopy(all_prefilled_outputs) if all_prefilled_outputs is not None else None,\n",
    ")\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "\n",
    "# Calculate audio duration and additional metrics\n",
    "if outputs.speech_outputs and outputs.speech_outputs[0] is not None:\n",
    "    # Assuming 24kHz sample rate (common for speech synthesis)\n",
    "    sample_rate = 24000\n",
    "    audio_samples = outputs.speech_outputs[0].shape[-1] if len(outputs.speech_outputs[0].shape) > 0 else len(outputs.speech_outputs[0])\n",
    "    audio_duration = audio_samples / sample_rate\n",
    "    rtf = generation_time / audio_duration if audio_duration > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Generated audio duration: {audio_duration:.2f} seconds\")\n",
    "    print(f\"RTF (Real Time Factor): {rtf:.2f}x\")\n",
    "else:\n",
    "    print(\"No audio output generated\")\n",
    "\n",
    "# Calculate token metrics\n",
    "input_tokens = inputs['tts_text_ids'].shape[1]  # Number of input tokens\n",
    "output_tokens = outputs.sequences.shape[1]  # Total tokens (input + generated)\n",
    "generated_tokens = output_tokens - input_tokens - all_prefilled_outputs['tts_lm']['last_hidden_state'].size(1)\n",
    "\n",
    "print(f\"Prefilling text tokens: {input_tokens}\")\n",
    "print(f\"Generated speech tokens: {generated_tokens}\")\n",
    "print(f\"Total tokens: {output_tokens}\")\n",
    "\n",
    "# Save output (processor handles device internally)\n",
    "txt_filename = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "output_path = os.path.join(output_dir, f\"{txt_filename}_generated.wav\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "processor.save_audio(\n",
    "    outputs.speech_outputs[0], # First (and only) batch item\n",
    "    output_path=output_path,\n",
    ")\n",
    "print(f\"Saved output to {output_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input file: {txt_path}\")\n",
    "print(f\"Output file: {output_path}\")\n",
    "print(f\"Speaker names: {speaker_name}\")\n",
    "print(f\"Prefilling text tokens: {input_tokens}\")\n",
    "print(f\"Generated speech tokens: {generated_tokens}\")\n",
    "print(f\"Total tokens: {output_tokens}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "print(f\"RTF (Real Time Factor): {rtf:.2f}x\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032fd94-ef97-4f88-a0ce-7d546f1aa681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
