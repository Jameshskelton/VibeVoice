{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80efabf3",
   "metadata": {},
   "source": [
    "# How to use to generate LiveAvatar script\n",
    "\n",
    "1. First, you need a VLLM server running a compatible LLM. We recommend using gpt-oss 120b\n",
    "2. run `pip install transformers==4.41.1 torch torchaudio`\n",
    "3. run `bash demo/download_experimental_voices.sh`\n",
    "\n",
    "## Run cells in order to get .wav file script for LiveAvatar\n",
    "\n",
    "Output will be saved to this folder as `file_generated.wav`\n",
    "\n",
    "Once complete, navigate to run LiveAvatar using DigitalOcean Gradient 8 x NVIDIA H100 or 8 x NVIDIA H200 GPU Droplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de54412",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.51.3\n",
    "!pip install diffusers\n",
    "!pip install torch torchaudio\n",
    "!bash demo/download_experimental_voices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df40d46-1717-4221-96af-0f529112c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[upbeat intro music]  \n",
      "Welcome to a quick dive into turning DigitalOcean’s 1‑Click Model GPU droplets into your very own voice‑enabled personal assistant.  \n",
      "\n",
      "First, what are 1‑Click Models? [curious tone] They’re a collaboration between DigitalOcean and Hugging Face that lets you spin up a cloud GPU droplet pre‑loaded with powerful open‑source LLMs—no coding required. Choose from models like Meta‑Llama‑3.1, Gemma‑2, Mixtral, or Hermes, all ready to run on the droplet’s GPU.  \n",
      "\n",
      "Once your droplet is up, you’ll see a Bearer Token in the SSH welcome message. [pause] Save that token—it’s the key to sending requests to the model’s inference API on port 8080, whether you’re on the same machine or a remote one.  \n",
      "\n",
      "You can query the model with a simple cURL command:  \n",
      "```bash\n",
      "curl http://localhost:8080/v1/chat/completions -X POST \\\n",
      "  -d '{\"messages\":[{\"role\":\"user\",\"content\":\"What is Deep Learning?\"}], \"temperature\":0.7,\"top_p\":0.95,\"max_tokens\":128}' \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -H \"Authorization: Bearer $BEARER_TOKEN\"\n",
      "```  \n",
      "Or, use Python via the Hugging Face Hub client to get a ChatCompletion object.  \n",
      "\n",
      "Now, the fun part: building a voice‑first assistant. We combine three pieces:  \n",
      "1. **Whisper** for speech‑to‑text transcription,  \n",
      "2. The 1‑Click LLM for generating a response, and  \n",
      "3. **Coqui‑AI’s XTTS2** for text‑to‑speech, even cloning your own voice.  \n",
      "\n",
      "All of this is wrapped in a lightweight Gradio interface. The code launches a chatbot UI, lets you type or upload audio, and then streams the assistant’s spoken reply back to you.  \n",
      "\n",
      "To get it running, install the dependencies on your droplet:  \n",
      "```bash\n",
      "pip install gradio tts huggingface_hub transformers datasets scipy torch torchaudio\n",
      "```  \n",
      "Save the script as `app.py` and launch it with `python3 app.py`. Gradio will give you a shareable link, and you’ve got a full‑stack, cloud‑powered personal assistant—no proprietary API keys needed.  \n",
      "\n",
      "In our tests, this setup swaps out closed‑source tools like Gemini or ChatGPT, delivering comparable answers with the flexibility of open‑source models. It’s especially compelling for heavy‑weight LLMs like Mixtral‑8×22B or LLaMA‑405B, where renting a GPU droplet can be more cost‑effective than enterprise licenses.  \n",
      "\n",
      "So, whether you’re building a custom AI helper, a prototype, or just experimenting with the newest open‑source models, 1‑Click Model droplets make it surprisingly easy.  \n",
      "\n",
      "Thanks for watching! [cheerful outro music]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "title = 'Turning Your 1-Click Model GPU Droplets Into A Personal Assistant'\n",
    "article =\"\"\"\n",
    "Generating videos from text or still images is one of the truly incredible and unique applications of Deep Learning technology. From anything we can imagine, whether it is complete fantasy or mundane activity, we can now see what it is with the clacking of a few computer keys.\n",
    "\n",
    "Video has a sort of magic that images do not, and it imparts a realness that was hard to capture with prior technology, often even with advanced CGI. With image generators, pretty much everything we can think of can also be created, and created better with the application of time and work, with something like Photoshop. In that sense, Video generators are far more versatile, able to create complex changes, motions, and effects on unique subjects in motion rather than static.\n",
    "\n",
    "In this piece, we want to introduce the newest State of the Art in video generation for open source deep learning models: HunyuanVideo 1.5. This model, released late last week, is on par with closed source models like Wan2.5 and Sora 2 without any of the closed source related issues that may limit use of or access to the model.\n",
    "\n",
    "With DigitalOcean’s Gradient GPU Droplets, it is easy to run HunyuanVideo 1.5 with the popular ComfyUI and DiffSynth-Studio] tools. In this article, we take a look at what makes HunyuanVideo so powerful, and then show how to run the model with Gradient. For the demo, we will walk through the steps required for everything to run the model on an NVIDIA H200 powered GPU Droplet using the ComfyUI.\n",
    "Prerequisites\n",
    "\n",
    "    access to an NVIDIA GPU Droplet\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "    HunyuanVideo 1.5 is a suite of text-to-video, image-to-video, and video Super Resolution models on par with the best closed source models like Wan2.5 and Sora 2\n",
    "    With only 8.3 billion parameters, the model is capable of being efficiently run for inference on consumer-grade GPUs\n",
    "    With Gradient GPU Droplets powered by an NVIDIA H200, we can generate 720p videos in minutes\n",
    "\n",
    "HunyuanVideo 1.5\n",
    "\n",
    "HunyuanVideo 1.5 is a lightweight yet robust video generation system that delivers SOTA visual quality and motion coherence using only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. Its performance is the result of several key components: rigorous data curation, an advanced DiT architecture incorporating selective and sliding tile attention (SSTA), improved bilingual capability through glyph-aware text encoding, a progressive pre-training and post-training strategy, and a highly efficient video super-resolution module. Together, these elements form a unified framework that supports high-quality text-to-video and image-to-video generation across a wide range of durations and resolutions.\n",
    "Training\n",
    "\n",
    "The training of HunyuanVideo 1.5 is defined by two key traits: rigorous data curation and using the Muon optimizer. During Data Acquisition, they prioritized data diversity and quality. They sourced from a variety of video channels, and then optimized the data for training efficiency by segmenting the data into 2-10 second clips. They then filtered their data for visual quality, aesthetics, and basic features like video borders.\n",
    "\n",
    "To caption the videos they used the same process as HunyuanImage 3.0, involving “(1) a hierarchical schema for structured image description, (2) a compositional synthesis strategy for diverse data augmentation, and (3) specialized agents for factual grounding.” (Source). Together, this creates a robust system for effectively and efficiently captioning each video for subsequent training.\n",
    "\n",
    "Finally, we get to the actual training. They approached training in 3 stages. They first trained for the text-to-image (t2i) task, at 256p and then 512p. The t2i training allowed the model to learn semantic alignment between text and images. They found that this effectively improves model training by accelerating the convergence and performance of subsequent text to video (t2v) and image to video (i2v) stages.\n",
    "\n",
    "During pretraining, they utilize a blended training approach that integrates T2I, T2V, and I2V tasks in a 1:6:3 ratio, balancing semantic depth and video-specific modeling. Large-scale T2I datasets are prioritized to enrich the model’s understanding of visual semantics and expand generative diversity, while T2V and I2V tasks ensure robust video-specific capabilities. A structured, multi-stage progression (Stages III to VI in Table 2) is employed, beginning at 256p resolution with 16 fps and gradually escalating to 480p and 720p at 24 fps, with video durations spanning 2 to 10 seconds. This gradual increase in spatiotemporal resolution fosters stable convergence and enhances the model’s ability to produce detailed, coherent video outputs. (Source). For Post-training, they implement a series of intertwined stages of continuing training, reinforcement learning, and supervised fine-tuning applied separately for i2v and t2v tasks. Eventually, these stages lead to the final resultant models of i2v and t2v models.\n",
    "Architecture\n",
    "\n",
    "image\n",
    "\n",
    "Above we can see the unified Diffusion Transformer Architecture. This outlines the path the model takes to generate an image during inference. For example, “for the I2V task, the reference image is integrated into the model via two complementary strategies: (1) VAE-based encoding, where the image latent is concatenated with the noisy latent along the channel dimension to leverage its exceptional detail reconstruction capacity; and (2) SigLip-based feature extraction, where semantic embeddings are concatenated sequentially to enhance semantic alignment and strengthen instruction adherence in I2V generation. A learnable type embedding is introduced to explicitly distinguish between different types of conditions.” (Source).\n",
    "\n",
    "The Variational AutoEncoder (VAE) is a “causal 3D transformer architecture designed for joint image-video encoding, which achieves a spatial compression ratio of (16 \\times) and a temporal compression ratio of (4 \\times), with a latent channel dimension of 32.” The text encoder is a Multimodal LLM (MLLM) leveraging Qwen 2.5 VL as a multimodal encoder. The further integration of Glyph ByT5 strengthens the model’s ability to understand and render text across different languages. Finally, they also use SigLip to align images and text in a shared representation space for tasks such as zero-shot image classification and image-text retrieval.\n",
    "\n",
    "To handle all of this data across multiple modalities, they use a novel attention mechanism they coin the Selective and Sliding Tile Attention (SSTA). “The SSTA algorithm comprises four key steps: 3D Block Partition, Selective Mask Generation, STA Mask Generation and Block-Sparse Attention. They propose an engineered acceleration toolkit for sparse attention mechanisms, utilizing the ThunderKittens framework to efficiently implement the flex_block_attention algorithm.” (Source).\n",
    "How to run HunyuanVideo 1.5 on a Gradient GPU Droplet\n",
    "\n",
    "To get started with running HunyuanVideo 1.5 on a Gradient GPU Droplet, we recommend following this tutorial. It will outline all the steps required to get your GPU Droplet spun up with SSH access, and then discusses how to set up VS Code/Cursor to use the Simple Browser feature to access the ComfyUI (running on your cloud machine’s GPU) in your local browser. We recommend an NVIDIA H200 GPU for this tutorial.\n",
    "\n",
    "Once you have spun up the GPU Droplet following the tutorial, access it from your local terminal using SSH. Change into the working directory of your choice, and then paste the following code into the terminal. It will clone the ComfyUI repo, download the required models, and run the ComfyUI launch command.\n",
    "\n",
    "git clone https://github.com/comfyanonymous/ComfyUI\n",
    "cd ComfyUI\n",
    "apt install python3-venv python3-pip\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "cd models/clip_vision\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/clip_vision/sigclip_vision_patch14_384.safetensors\n",
    "cd ../text_encoders\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/byt5_small_glyphxl_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b.safetensors\n",
    "cd ../vae\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/vae/hunyuanvideo15_vae_fp16.safetensors\n",
    "cd ../diffusion_models\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_t2v_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_i2v_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors\n",
    "cd ../..\n",
    "python main.py\n",
    "\n",
    "Then take the URL output from the terminal, and paste the value into your Simple Browser on VS Code or Cursor. Then click the arrow button in the top right corner to open the ComfyUI in your browser. Then, download the workflow json from the ComfyUI Examples page (or click here), and open it in the ComfyUI (For the Image to Video workflow, click here). If everything worked, you should see something like shown below:\n",
    "\n",
    "image\n",
    "\n",
    "You can now get started generating videos by inputting your prompt. Change the height, width, step count, and number of frames values to make additional changes to the output. This workflow also includes the ability to do Video Super-Resolution upscaling if you “bypass” all the purple blanked out modules in the lower part of the workflow. If you run the workflow, you should get the following video output:\n",
    "\n",
    "gif\n",
    "\n",
    "As we can see the quality is fantastic, even on this down-scaled gif version of the original. Overall, This is a wonderful model for generating videos in all styles, including 3d, animation, realism, and more. On an H200, it can generate these videos in minutes. We highly recommend using the ComfyUI to generate videos with HunyuanVideo 1.5.\n",
    "How to run HunyuanVideo 1.5 on a Gradient GPU Droplet\n",
    "\n",
    "To get started with running HunyuanVideo 1.5 on a Gradient GPU Droplet, we recommend following this tutorial. It will outline all the steps required to get your GPU Droplet spun up with SSH access, and then discusses how to set up VS Code/Cursor to use the Simple Browser feature to access the ComfyUI (running on your cloud machine’s GPU) in your local browser. We recommend an NVIDIA H200 GPU for this tutorial.\n",
    "\n",
    "Once you have spun up the GPU Droplet following the tutorial, access it from your local terminal using SSH. Change into the working directory of your choice, and then paste the following code into the terminal. It will clone the ComfyUI repo, download the required models, and run the ComfyUI launch command.\n",
    "\n",
    "git clone https://github.com/comfyanonymous/ComfyUI\n",
    "cd ComfyUI\n",
    "apt install python3-venv python3-pip\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "cd models/clip_vision\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/clip_vision/sigclip_vision_patch14_384.safetensors\n",
    "cd ../text_encoders\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/byt5_small_glyphxl_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b.safetensors\n",
    "cd ../vae\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/vae/hunyuanvideo15_vae_fp16.safetensors\n",
    "cd ../diffusion_models\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_t2v_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_i2v_fp16.safetensors\n",
    "wget https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors\n",
    "cd ../..\n",
    "python main.py\n",
    "\n",
    "Then take the URL output from the terminal, and paste the value into your Simple Browser on VS Code or Cursor. Then click the arrow button in the top right corner to open the ComfyUI in your browser. Then, download the workflow json from the ComfyUI Examples page (or click here), and open it in the ComfyUI (For the Image to Video workflow, click here). If everything worked, you should see something like shown below:\n",
    "\n",
    "image\n",
    "\n",
    "You can now get started generating videos by inputting your prompt. Change the height, width, step count, and number of frames values to make additional changes to the output. This workflow also includes the ability to do Video Super-Resolution upscaling if you “bypass” all the purple blanked out modules in the lower part of the workflow. If you run the workflow, you should get the following video output:\n",
    "\n",
    "gif\n",
    "\n",
    "As we can see the quality is fantastic, even on this down-scaled gif version of the original. Overall, This is a wonderful model for generating videos in all styles, including 3d, animation, realism, and more. On an H200, it can generate these videos in minutes. We highly recommend using the ComfyUI to generate videos with HunyuanVideo 1.5.\n",
    "Closing Thoughts\n",
    "\n",
    "HunyuanVideo 1.5 is a fantastic video model with capabilities that rival models like Sora 2 in pure video generation capabilities. Thanks to the innovative training strategy, we expect future releases to be even more monumental in their impact on the open-source video generation scene. We encourage you all to try this model on Gradient today!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " \n",
    "result = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for creating short video essay scripts.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Extract the content from the following article (which includes all text following the colon), and create a short summary script for a video explaining the article. Do not include any additional text for the script, only the words that are to be spoken. The article is titled {title}, and its content is: {article}\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(result.choices[0].message.content)\n",
    "text = str(result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab288e46-830b-4b29-bd89-834c6e0bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.txt', 'w') as file:\n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9b5c2a-499e-439d-98cd-877956ec5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APEX FusedRMSNorm not available, using native implementation\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from typing import List, Tuple, Union, Dict, Any\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "import glob\n",
    "\n",
    "from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference\n",
    "from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor\n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class VoiceMapper:\n",
    "    \"\"\"Maps speaker names to voice file paths\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_voice_presets()\n",
    "        for k, v in self.voice_presets.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    def setup_voice_presets(self):\n",
    "        \"\"\"Setup voice presets by scanning the voices directory.\"\"\"\n",
    "        voices_dir = os.path.join(os.path.dirname('./demo/'), \"voices/streaming_model\")\n",
    "        \n",
    "        # Check if voices directory exists\n",
    "        if not os.path.exists(voices_dir):\n",
    "            print(f\"Warning: Voices directory not found at {voices_dir}\")\n",
    "            self.voice_presets = {}\n",
    "            self.available_voices = {}\n",
    "            return\n",
    "        \n",
    "        # Scan for all VOICE files in the voices directory\n",
    "        self.voice_presets = {}\n",
    "        \n",
    "        # Get all .pt files in the voices directory\n",
    "        pt_files = glob.glob(os.path.join(voices_dir, \"**\", \"*.pt\"), recursive=True)\n",
    "        \n",
    "        # Create dictionary with filename (without extension) as key\n",
    "        for pt_file in pt_files:\n",
    "            # key: filename without extension\n",
    "            name = os.path.splitext(os.path.basename(pt_file))[0].lower()\n",
    "            full_path = os.path.abspath(pt_file)\n",
    "            self.voice_presets[name] = full_path\n",
    "        \n",
    "        # Sort the voice presets alphabetically by name for better UI\n",
    "        self.voice_presets = dict(sorted(self.voice_presets.items()))\n",
    "        \n",
    "        # Filter out voices that don't exist (this is now redundant but kept for safety)\n",
    "        self.available_voices = {\n",
    "            name: path for name, path in self.voice_presets.items()\n",
    "            if os.path.exists(path)\n",
    "        }\n",
    "        \n",
    "        print(f\"Found {len(self.available_voices)} voice files in {voices_dir}\")\n",
    "        print(f\"Available voices: {', '.join(self.available_voices.keys())}\")\n",
    "\n",
    "    def get_voice_path(self, speaker_name: str) -> str:\n",
    "        \"\"\"Get voice file path for a given speaker name\"\"\"\n",
    "        # First try exact match\n",
    "        speaker_name = speaker_name.lower()\n",
    "        if speaker_name in self.voice_presets:\n",
    "            return self.voice_presets[speaker_name]\n",
    "        \n",
    "        # Try partial matching (case insensitive)\n",
    "        matched_path = None\n",
    "        for preset_name, path in self.voice_presets.items():\n",
    "            if preset_name.lower() in speaker_name or speaker_name in preset_name.lower():\n",
    "                if matched_path is not None:\n",
    "                    raise ValueError(f\"Multiple voice presets match the speaker name '{speaker_name}', please make the speaker_name more specific.\")\n",
    "                matched_path = path\n",
    "        if matched_path is not None:\n",
    "            return matched_path\n",
    "        \n",
    "        # Default to first voice if no match found\n",
    "        default_voice = list(self.voice_presets.values())[0]\n",
    "        print(f\"Warning: No voice preset found for '{speaker_name}', using default voice: {default_voice}\")\n",
    "        return default_voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183f2324-68ff-4c4a-8ac1-9a9438140941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/060db6499f32faf8b98477b0a26969ef7d8b9987/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
      "The class this function is called from is 'VibeVoiceTextTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 voice files in ./demo/voices/streaming_model\n",
      "Available voices: de-spk0_man, de-spk1_woman, de-spk2_woman, de-spk3_man, de-spk4_woman, de-spk5_man, de-spk6_man, en-breeze_woman, en-brutalon_man, en-carter_man, en-clarion_man, en-clarissa_woman, en-davis_man, en-emma_woman, en-frank_man, en-grace_woman, en-gravitar_man, en-gravus_man, en-mechcorsair_man, en-mike_man, en-oldenheart_man, en-silkvox_man, en-snarkling_woman, en-soother_woman, fr-spk0_man, fr-spk1_woman, fr-spk2_man, fr-spk3_woman, fr-spk4_woman, fr-spk5_man, in-samuel_man, it-spk0_woman, it-spk1_man, jp-spk0_man, jp-spk1_woman, jp-spk2_woman, jp-spk3_woman, jp-spk4_woman, jp-spk5_man, kr-spk0_woman, kr-spk1_man, kr-spk2_woman, kr-spk3_man, nl-spk0_man, nl-spk1_woman, pl-spk0_man, pl-spk1_woman, pl-spk2_man, pl-spk3_woman, pt-spk0_woman, pt-spk1_man, pt-spk2_woman, pt-spk3_man, pt-spk4_man, pt-spk5_woman, sp-spk0_woman, sp-spk1_man, sp-spk2_woman, sp-spk3_man, sp-spk4_woman, sp-spk5_man\n",
      "de-spk0_man: /home/VibeVoice/demo/voices/streaming_model/de-Spk0_man.pt\n",
      "de-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/de-Spk1_woman.pt\n",
      "de-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk2_woman.pt\n",
      "de-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk3_man.pt\n",
      "de-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk4_woman.pt\n",
      "de-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk5_man.pt\n",
      "de-spk6_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/de/de-Spk6_man.pt\n",
      "en-breeze_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Breeze_woman.pt\n",
      "en-brutalon_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Brutalon_man.pt\n",
      "en-carter_man: /home/VibeVoice/demo/voices/streaming_model/en-Carter_man.pt\n",
      "en-clarion_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Clarion_man.pt\n",
      "en-clarissa_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Clarissa_woman.pt\n",
      "en-davis_man: /home/VibeVoice/demo/voices/streaming_model/en-Davis_man.pt\n",
      "en-emma_woman: /home/VibeVoice/demo/voices/streaming_model/en-Emma_woman.pt\n",
      "en-frank_man: /home/VibeVoice/demo/voices/streaming_model/en-Frank_man.pt\n",
      "en-grace_woman: /home/VibeVoice/demo/voices/streaming_model/en-Grace_woman.pt\n",
      "en-gravitar_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Gravitar_man.pt\n",
      "en-gravus_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Gravus_man.pt\n",
      "en-mechcorsair_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-MechCorsair_man.pt\n",
      "en-mike_man: /home/VibeVoice/demo/voices/streaming_model/en-Mike_man.pt\n",
      "en-oldenheart_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Oldenheart_man.pt\n",
      "en-silkvox_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Silkvox_man.pt\n",
      "en-snarkling_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Snarkling_woman.pt\n",
      "en-soother_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/en/en-Soother_woman.pt\n",
      "fr-spk0_man: /home/VibeVoice/demo/voices/streaming_model/fr-Spk0_man.pt\n",
      "fr-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/fr-Spk1_woman.pt\n",
      "fr-spk2_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk2_man.pt\n",
      "fr-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk3_woman.pt\n",
      "fr-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk4_woman.pt\n",
      "fr-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/fr/fr-Spk5_man.pt\n",
      "in-samuel_man: /home/VibeVoice/demo/voices/streaming_model/in-Samuel_man.pt\n",
      "it-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/it-Spk0_woman.pt\n",
      "it-spk1_man: /home/VibeVoice/demo/voices/streaming_model/it-Spk1_man.pt\n",
      "jp-spk0_man: /home/VibeVoice/demo/voices/streaming_model/jp-Spk0_man.pt\n",
      "jp-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/jp-Spk1_woman.pt\n",
      "jp-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk2_woman.pt\n",
      "jp-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk3_woman.pt\n",
      "jp-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk4_woman.pt\n",
      "jp-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/jp/jp-Spk5_man.pt\n",
      "kr-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/kr-Spk0_woman.pt\n",
      "kr-spk1_man: /home/VibeVoice/demo/voices/streaming_model/kr-Spk1_man.pt\n",
      "kr-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/kr/kr-Spk2_woman.pt\n",
      "kr-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/kr/kr-Spk3_man.pt\n",
      "nl-spk0_man: /home/VibeVoice/demo/voices/streaming_model/nl-Spk0_man.pt\n",
      "nl-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/nl-Spk1_woman.pt\n",
      "pl-spk0_man: /home/VibeVoice/demo/voices/streaming_model/pl-Spk0_man.pt\n",
      "pl-spk1_woman: /home/VibeVoice/demo/voices/streaming_model/pl-Spk1_woman.pt\n",
      "pl-spk2_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pl/pl-Spk2_man.pt\n",
      "pl-spk3_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pl/pl-Spk3_woman.pt\n",
      "pt-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/pt-Spk0_woman.pt\n",
      "pt-spk1_man: /home/VibeVoice/demo/voices/streaming_model/pt-Spk1_man.pt\n",
      "pt-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk2_woman.pt\n",
      "pt-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk3_man.pt\n",
      "pt-spk4_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk4_man.pt\n",
      "pt-spk5_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/pt/pt-Spk5_woman.pt\n",
      "sp-spk0_woman: /home/VibeVoice/demo/voices/streaming_model/sp-Spk0_woman.pt\n",
      "sp-spk1_man: /home/VibeVoice/demo/voices/streaming_model/sp-Spk1_man.pt\n",
      "sp-spk2_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk2_woman.pt\n",
      "sp-spk3_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk3_man.pt\n",
      "sp-spk4_woman: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk4_woman.pt\n",
      "sp-spk5_man: /home/VibeVoice/demo/voices/streaming_model/experimental_voices/sp/sp-Spk5_man.pt\n",
      "Reading script from: file.txt\n",
      "Loading processor & model from microsoft/VibeVoice-Realtime-0.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/config.json\n",
      "Model config VibeVoiceStreamingConfig {\n",
      "  \"acoustic_tokenizer_config\": {\n",
      "    \"causal\": true,\n",
      "    \"channels\": 1,\n",
      "    \"conv_bias\": true,\n",
      "    \"conv_norm\": \"none\",\n",
      "    \"corpus_normalize\": 0.0,\n",
      "    \"decoder_depths\": null,\n",
      "    \"decoder_n_filters\": 32,\n",
      "    \"decoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"disable_last_norm\": true,\n",
      "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
      "    \"encoder_n_filters\": 32,\n",
      "    \"encoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"fix_std\": 0.5,\n",
      "    \"layer_scale_init_value\": 1e-06,\n",
      "    \"layernorm\": \"RMSNorm\",\n",
      "    \"layernorm_elementwise_affine\": true,\n",
      "    \"layernorm_eps\": 1e-05,\n",
      "    \"mixer_layer\": \"depthwise_conv\",\n",
      "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
      "    \"pad_mode\": \"constant\",\n",
      "    \"std_dist_type\": \"gaussian\",\n",
      "    \"vae_dim\": 64,\n",
      "    \"weight_init_value\": 0.01\n",
      "  },\n",
      "  \"acoustic_vae_dim\": 64,\n",
      "  \"architectures\": [\n",
      "    \"VibeVoiceStreamingForConditionalGenerationInference\"\n",
      "  ],\n",
      "  \"decoder_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 896,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4864,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"max_window_layers\": 24,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"num_attention_heads\": 14,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"diffusion_head_config\": {\n",
      "    \"ddpm_batch_mul\": 4,\n",
      "    \"ddpm_beta_schedule\": \"cosine\",\n",
      "    \"ddpm_num_inference_steps\": 20,\n",
      "    \"ddpm_num_steps\": 1000,\n",
      "    \"diffusion_type\": \"ddpm\",\n",
      "    \"head_ffn_ratio\": 3.0,\n",
      "    \"head_layers\": 4,\n",
      "    \"hidden_size\": 896,\n",
      "    \"latent_size\": 64,\n",
      "    \"model_type\": \"vibevoice_diffusion_head\",\n",
      "    \"prediction_type\": \"v_prediction\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"speech_vae_dim\": 64\n",
      "  },\n",
      "  \"model_type\": \"vibevoice_streaming\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tts_backbone_num_hidden_layers\": 20\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/model.safetensors\n",
      "Instantiating VibeVoiceStreamingForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/config.json\n",
      "Model config VibeVoiceStreamingConfig {\n",
      "  \"acoustic_tokenizer_config\": {\n",
      "    \"causal\": true,\n",
      "    \"channels\": 1,\n",
      "    \"conv_bias\": true,\n",
      "    \"conv_norm\": \"none\",\n",
      "    \"corpus_normalize\": 0.0,\n",
      "    \"decoder_depths\": null,\n",
      "    \"decoder_n_filters\": 32,\n",
      "    \"decoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"disable_last_norm\": true,\n",
      "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
      "    \"encoder_n_filters\": 32,\n",
      "    \"encoder_ratios\": [\n",
      "      8,\n",
      "      5,\n",
      "      5,\n",
      "      4,\n",
      "      2,\n",
      "      2\n",
      "    ],\n",
      "    \"fix_std\": 0.5,\n",
      "    \"layer_scale_init_value\": 1e-06,\n",
      "    \"layernorm\": \"RMSNorm\",\n",
      "    \"layernorm_elementwise_affine\": true,\n",
      "    \"layernorm_eps\": 1e-05,\n",
      "    \"mixer_layer\": \"depthwise_conv\",\n",
      "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
      "    \"pad_mode\": \"constant\",\n",
      "    \"std_dist_type\": \"gaussian\",\n",
      "    \"vae_dim\": 64,\n",
      "    \"weight_init_value\": 0.01\n",
      "  },\n",
      "  \"acoustic_vae_dim\": 64,\n",
      "  \"architectures\": [\n",
      "    \"VibeVoiceStreamingForConditionalGenerationInference\"\n",
      "  ],\n",
      "  \"decoder_config\": {\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 896,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4864,\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"max_window_layers\": 24,\n",
      "    \"model_type\": \"qwen2\",\n",
      "    \"num_attention_heads\": 14,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"torch_dtype\": \"bfloat16\",\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"diffusion_head_config\": {\n",
      "    \"ddpm_batch_mul\": 4,\n",
      "    \"ddpm_beta_schedule\": \"cosine\",\n",
      "    \"ddpm_num_inference_steps\": 20,\n",
      "    \"ddpm_num_steps\": 1000,\n",
      "    \"diffusion_type\": \"ddpm\",\n",
      "    \"head_ffn_ratio\": 3.0,\n",
      "    \"head_layers\": 4,\n",
      "    \"hidden_size\": 896,\n",
      "    \"latent_size\": 64,\n",
      "    \"model_type\": \"vibevoice_diffusion_head\",\n",
      "    \"prediction_type\": \"v_prediction\",\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"speech_vae_dim\": 64\n",
      "  },\n",
      "  \"model_type\": \"vibevoice_streaming\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"tts_backbone_num_hidden_layers\": 20\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--VibeVoice-Realtime-0.5B/snapshots/6bce5f06044837fe6d2c5d7a71a84f0416bd57e4/model.safetensors\n",
      "Instantiating VibeVoiceStreamingForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {}\n",
      "\n",
      "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
      "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
      "Instantiating VibeVoiceAcousticTokenizerModel model under default dtype torch.bfloat16.\n",
      "Instantiating VibeVoiceDiffusionHead model under default dtype torch.bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, torch_dtype: torch.bfloat16, attn_implementation: flash_attention_2\n",
      "[ERROR] : ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_437806/4057761752.py\", line 50, in <module>\n",
      "    model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4336, in from_pretrained\n",
      "    config = cls._autoset_attn_implementation(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2109, in _autoset_attn_implementation\n",
      "    cls._check_and_enable_flash_attn_2(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2252, in _check_and_enable_flash_attn_2\n",
      "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
      "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "\n",
      "Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing VibeVoiceStreamingForConditionalGenerationInference.\n",
      "\n",
      "All the weights of VibeVoiceStreamingForConditionalGenerationInference were initialized from the model checkpoint at microsoft/VibeVoice-Realtime-0.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use VibeVoiceStreamingForConditionalGenerationInference for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model attention: sdpa\n",
      "Using voice preset for en-carter_man: /home/VibeVoice/demo/voices/streaming_model/en-Carter_man.pt\n",
      "Starting generation with cfg_scale: 1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 83.14 seconds\n",
      "Generated audio duration: 266.40 seconds\n",
      "RTF (Real Time Factor): 0.31x\n",
      "Prefilling text tokens: 646\n",
      "Generated speech tokens: 1998\n",
      "Total tokens: 2960\n",
      "Saved output to ./file_generated.wav\n",
      "\n",
      "==================================================\n",
      "GENERATION SUMMARY\n",
      "==================================================\n",
      "Input file: file.txt\n",
      "Output file: ./file_generated.wav\n",
      "Speaker names: en-carter_man\n",
      "Prefilling text tokens: 646\n",
      "Generated speech tokens: 1998\n",
      "Total tokens: 2960\n",
      "Generation time: 83.14 seconds\n",
      "Audio duration: 266.40 seconds\n",
      "RTF (Real Time Factor): 0.31x\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = \"microsoft/VibeVoice-Realtime-0.5B\"\n",
    "txt_path = \"file.txt\"\n",
    "speaker_name = \"en-carter_man\"\n",
    "ouput_dir = \"./outputs\"\n",
    "device = 'cuda'\n",
    "cfg_scale = 1.5\n",
    "output_dir = './'\n",
    "# Initialize voice mapper\n",
    "voice_mapper = VoiceMapper()\n",
    "\n",
    "# Check if txt file exists\n",
    "if not os.path.exists(txt_path):\n",
    "    print(f\"Error: txt file not found: {txt_path}\")\n",
    "\n",
    "# Read and parse txt file\n",
    "print(f\"Reading script from: {txt_path}\")\n",
    "with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "    scripts = f.read().strip()\n",
    "\n",
    "if not scripts:\n",
    "    print(\"Error: No valid scripts found in the txt file\")\n",
    "\n",
    "full_script = scripts.replace(\"’\", \"'\").replace('“', '\"').replace('”', '\"')\n",
    "\n",
    "print(f\"Loading processor & model from {model_path}\")\n",
    "processor = VibeVoiceStreamingProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Decide dtype & attention implementation\n",
    "if device == \"mps\":\n",
    "    load_dtype = torch.float32  # MPS requires float32\n",
    "    attn_impl_primary = \"sdpa\"  # flash_attention_2 not supported on MPS\n",
    "elif device == \"cuda\":\n",
    "    load_dtype = torch.bfloat16\n",
    "    attn_impl_primary = \"flash_attention_2\"\n",
    "else:  # cpu\n",
    "    load_dtype = torch.float32\n",
    "    attn_impl_primary = \"sdpa\"\n",
    "print(f\"Using device: {device}, torch_dtype: {load_dtype}, attn_implementation: {attn_impl_primary}\")\n",
    "# Load model with device-specific logic\n",
    "try:\n",
    "    if device == \"mps\":\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            attn_implementation=attn_impl_primary,\n",
    "            device_map=None,  # load then move\n",
    "        )\n",
    "        model.to(\"mps\")\n",
    "    elif device == \"cuda\":\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=\"cuda\",\n",
    "            attn_implementation=attn_impl_primary,\n",
    "        )\n",
    "    else:  # cpu\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=\"cpu\",\n",
    "            attn_implementation=attn_impl_primary,\n",
    "        )\n",
    "except Exception as e:\n",
    "    if attn_impl_primary == 'flash_attention_2':\n",
    "        print(f\"[ERROR] : {type(e).__name__}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.\")\n",
    "        model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=load_dtype,\n",
    "            device_map=(device if device in (\"cuda\", \"cpu\") else None),\n",
    "            attn_implementation='sdpa'\n",
    "        )\n",
    "        if device == \"mps\":\n",
    "            model.to(\"mps\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.set_ddpm_inference_steps(num_steps=5)\n",
    "\n",
    "if hasattr(model.model, 'language_model'):\n",
    "   print(f\"Language model attention: {model.model.language_model.config._attn_implementation}\")\n",
    "\n",
    "target_device = device if device != \"cpu\" else \"cpu\"\n",
    "voice_sample = voice_mapper.get_voice_path(speaker_name)\n",
    "print(f\"Using voice preset for {speaker_name}: {voice_sample}\")\n",
    "all_prefilled_outputs = torch.load(voice_sample, map_location=target_device, weights_only=False)\n",
    "\n",
    "# Prepare inputs for the model\n",
    "inputs = processor.process_input_with_cached_prompt(\n",
    "    text=full_script,\n",
    "    cached_prompt=all_prefilled_outputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "# Move tensors to target device\n",
    "for k, v in inputs.items():\n",
    "    if torch.is_tensor(v):\n",
    "        inputs[k] = v.to(target_device)\n",
    "\n",
    "print(f\"Starting generation with cfg_scale: {cfg_scale}\")\n",
    "\n",
    "# Generate audio\n",
    "start_time = time.time()\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=None,\n",
    "    cfg_scale=cfg_scale,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    generation_config={'do_sample': False},\n",
    "    verbose=True,\n",
    "    all_prefilled_outputs=copy.deepcopy(all_prefilled_outputs) if all_prefilled_outputs is not None else None,\n",
    ")\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "\n",
    "# Calculate audio duration and additional metrics\n",
    "if outputs.speech_outputs and outputs.speech_outputs[0] is not None:\n",
    "    # Assuming 24kHz sample rate (common for speech synthesis)\n",
    "    sample_rate = 24000\n",
    "    audio_samples = outputs.speech_outputs[0].shape[-1] if len(outputs.speech_outputs[0].shape) > 0 else len(outputs.speech_outputs[0])\n",
    "    audio_duration = audio_samples / sample_rate\n",
    "    rtf = generation_time / audio_duration if audio_duration > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Generated audio duration: {audio_duration:.2f} seconds\")\n",
    "    print(f\"RTF (Real Time Factor): {rtf:.2f}x\")\n",
    "else:\n",
    "    print(\"No audio output generated\")\n",
    "\n",
    "# Calculate token metrics\n",
    "input_tokens = inputs['tts_text_ids'].shape[1]  # Number of input tokens\n",
    "output_tokens = outputs.sequences.shape[1]  # Total tokens (input + generated)\n",
    "generated_tokens = output_tokens - input_tokens - all_prefilled_outputs['tts_lm']['last_hidden_state'].size(1)\n",
    "\n",
    "print(f\"Prefilling text tokens: {input_tokens}\")\n",
    "print(f\"Generated speech tokens: {generated_tokens}\")\n",
    "print(f\"Total tokens: {output_tokens}\")\n",
    "\n",
    "# Save output (processor handles device internally)\n",
    "txt_filename = os.path.splitext(os.path.basename(txt_path))[0]\n",
    "output_path = os.path.join(output_dir, f\"{txt_filename}_generated.wav\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "processor.save_audio(\n",
    "    outputs.speech_outputs[0], # First (and only) batch item\n",
    "    output_path=output_path,\n",
    ")\n",
    "print(f\"Saved output to {output_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input file: {txt_path}\")\n",
    "print(f\"Output file: {output_path}\")\n",
    "print(f\"Speaker names: {speaker_name}\")\n",
    "print(f\"Prefilling text tokens: {input_tokens}\")\n",
    "print(f\"Generated speech tokens: {generated_tokens}\")\n",
    "print(f\"Total tokens: {output_tokens}\")\n",
    "print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "print(f\"RTF (Real Time Factor): {rtf:.2f}x\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032fd94-ef97-4f88-a0ce-7d546f1aa681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
